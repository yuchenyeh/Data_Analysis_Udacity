{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x0000000003A11F48>\n",
      "--------------\n",
      "MOUNTAIN VIEW MOFFETT FLD NAS\n",
      "--------------\n",
      "['Date (MM/DD/YYYY)', 'Time (HH:MM)', 'ETR (W/m^2)', 'ETRN (W/m^2)', 'GHI (W/m^2)', 'GHI source', 'GHI uncert (%)', 'DNI (W/m^2)', 'DNI source', 'DNI uncert (%)', 'DHI (W/m^2)', 'DHI source', 'DHI uncert (%)', 'GH illum (lx)', 'GH illum source', 'Global illum uncert (%)', 'DN illum (lx)', 'DN illum source', 'DN illum uncert (%)', 'DH illum (lx)', 'DH illum source', 'DH illum uncert (%)', 'Zenith lum (cd/m^2)', 'Zenith lum source', 'Zenith lum uncert (%)', 'TotCld (tenths)', 'TotCld source', 'TotCld uncert (code)', 'OpqCld (tenths)', 'OpqCld source', 'OpqCld uncert (code)', 'Dry-bulb (C)', 'Dry-bulb source', 'Dry-bulb uncert (code)', 'Dew-point (C)', 'Dew-point source', 'Dew-point uncert (code)', 'RHum (%)', 'RHum source', 'RHum uncert (code)', 'Pressure (mbar)', 'Pressure source', 'Pressure uncert (code)', 'Wdir (degrees)', 'Wdir source', 'Wdir uncert (code)', 'Wspd (m/s)', 'Wspd source', 'Wspd uncert (code)', 'Hvis (m)', 'Hvis source', 'Hvis uncert (code)', 'CeilHgt (m)', 'CeilHgt source', 'CeilHgt uncert (code)', 'Pwat (cm)', 'Pwat source', 'Pwat uncert (code)', 'AOD (unitless)', 'AOD source', 'AOD uncert (code)', 'Alb (unitless)', 'Alb source', 'Alb uncert (code)', 'Lprecip depth (mm)', 'Lprecip quantity (hr)', 'Lprecip source', 'Lprecip uncert (code)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'01:00'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"745090.csv\"\n",
    "\n",
    "\n",
    "name = \"\"\n",
    "data = []\n",
    "with open(DATAFILE,'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    print reader\n",
    "    print \"--------------\"\n",
    "    name =reader.next()[1]\n",
    "    print name\n",
    "    print \"--------------\"\n",
    "    t=reader.next()\n",
    "    print t\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "    \n",
    "data[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'COAST', 2013, 8, 13, 17, 18779.025510000003], [u'EAST', 2013, 8, 5, 17, 2380.1654089999956], [u'FAR_WEST', 2013, 6, 26, 17, 2281.2722140000024], [u'NORTH', 2013, 8, 7, 17, 1544.7707140000005], [u'NORTH_C', 2013, 8, 7, 18, 24415.570226999993], [u'SOUTHERN', 2013, 8, 8, 16, 5494.157645], [u'SOUTH_C', 2013, 8, 8, 18, 11433.30491600001], [u'WEST', 2013, 8, 7, 17, 1862.6137649999998]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "workbook = xlrd.open_workbook(datafile)\n",
    "sheet = workbook.sheet_by_index(0)\n",
    "data = []\n",
    "for each in range(1,9):\n",
    "    cv=sheet.col_values(each, start_rowx=1, end_rowx=None)\n",
    "    maxval = max(cv)\n",
    "    maxpos = cv.index(maxval) + 1\n",
    "    cname = sheet.cell_value(0, each)\n",
    "    maxtime = sheet.cell_value(maxpos,0)\n",
    "    realtime = xlrd.xldate_as_tuple(maxtime,0)\n",
    "    line = [cname]\n",
    "    for each in range(0,4):\n",
    "        line.append(realtime[each])\n",
    "    line.append(maxval)\n",
    "    data.append(line)\n",
    "    # YOUR CODE HERE\n",
    "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
    "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
    "print data\n",
    "\n",
    "with open(outfile, \"w\") as f:\n",
    "   w = csv.writer(f, delimiter='|')\n",
    "   w.writerow([\"Station\",\"Year\",\"Month\",\"Day\",\"Hour\",\"Max Load\"])\n",
    "   for each in range(len(data)):\n",
    "       w.writerow(data[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'popular-viewed-1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-25d863119a12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-25d863119a12>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle_overview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"viewed\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-25d863119a12>\u001b[0m in \u001b[0;36marticle_overview\u001b[1;34m(kind, period)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0marticle_overview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mtitles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-25d863119a12>\u001b[0m in \u001b[0;36mget_from_file\u001b[1;34m(kind, period)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"popular-{0}-{1}.json\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'popular-viewed-1.json'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site \n",
    "and get your own API key. You will be able to complete this exercise in our UI\n",
    "without doing so, as we have provided a sample result.\n",
    "\n",
    "Your task is to process the saved file that represents the most popular\n",
    "articles (by view count) from the last day, and return the following data:\n",
    "- list of dictionaries, where the dictionary key is \"section\" and value is \"title\"\n",
    "- list of URLs for all media entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview function.\n",
    "The rest of functions are provided for your convenience, if you want to access\n",
    "the API by yourself.\n",
    "\"\"\"\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"\",\n",
    "            \"article\": \"\"}\n",
    "\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "\n",
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    titles = []\n",
    "    urls =[]\n",
    "    # YOUR CODE HERE\n",
    "    for article in data:\n",
    "        section = article[\"section\"]\n",
    "        title = article[\"title\"]\n",
    "        titles.append({section: title})\n",
    "        \n",
    "        if \"media\" in aritcle:\n",
    "            for mm in n[\"media-metadata\"]:\n",
    "                if mm[\"format\"] == \"Standard Thumbnail\":\n",
    "                    urls.append(mm[\"url\"])\n",
    "    return (titles, urls)\n",
    "\n",
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        print \"You need to register for NYTimes Developer account to run this program.\"\n",
    "        print \"See Intructor notes for information\"\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print \"Time period can be 1,7, 30 days only\"\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print \"kind can be only one of viewed/shared/emailed\"\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "\n",
    "\n",
    "def test():\n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    assert len(titles) == 20\n",
    "    assert len(urls) == 30\n",
    "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
    "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<select class=\"slcBox\" id=\"CarrierList\" name=\"CarrierList\" style=\"width:450px;\">\n",
      "<option selected=\"selected\" value=\"All\">All U.S. and Foreign Carriers</option>\n",
      "<option value=\"AllUS\">All U.S. Carriers</option>\n",
      "<option value=\"AllForeign\">All Foreign Carriers</option>\n",
      "<option value=\"FL\">AirTran Airways</option>\n",
      "<option value=\"AS\">Alaska Airlines </option>\n",
      "<option value=\"AA\">American Airlines </option>\n",
      "<option value=\"MQ\">American Eagle Airlines </option>\n",
      "<option value=\"5Y\">Atlas Air </option>\n",
      "<option value=\"DL\">Delta Air Lines </option>\n",
      "<option value=\"EV\">ExpressJet Airlines </option>\n",
      "<option value=\"F9\">Frontier Airlines </option>\n",
      "<option value=\"HA\">Hawaiian Airlines </option>\n",
      "<option value=\"B6\">JetBlue Airways</option>\n",
      "<option value=\"OO\">SkyWest Airlines </option>\n",
      "<option value=\"WN\">Southwest Airlines </option>\n",
      "<option value=\"NK\">Spirit Air Lines</option>\n",
      "<option value=\"US\">US Airways </option>\n",
      "<option value=\"UA\">United Air Lines </option>\n",
      "<option value=\"VX\">Virgin America</option>\n",
      "</select>\n",
      "---------------\n",
      "AllForeign\n",
      "---------------\n",
      "FL\n",
      "---------------\n",
      "AS\n",
      "---------------\n",
      "AA\n",
      "---------------\n",
      "MQ\n",
      "---------------\n",
      "5Y\n",
      "---------------\n",
      "DL\n",
      "---------------\n",
      "EV\n",
      "---------------\n",
      "F9\n",
      "---------------\n",
      "HA\n",
      "---------------\n",
      "B6\n",
      "---------------\n",
      "OO\n",
      "---------------\n",
      "WN\n",
      "---------------\n",
      "NK\n",
      "---------------\n",
      "US\n",
      "---------------\n",
      "UA\n",
      "---------------\n",
      "VX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef make_request(data):\\n    eventvalidation = data[\"eventvalidation\"]\\n    viewstate = data[\"viewstate\"]\\n    airport = data[\"airport\"]\\n    carrier = data[\"carrier\"]\\n\\n    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\\n                    data={\\'AirportList\\': airport,\\n                          \\'CarrierList\\': carrier,\\n                          \\'Submit\\': \\'Submit\\',\\n                          \"__EVENTTARGET\": \"\",\\n                          \"__EVENTARGUMENT\": \"\",\\n                          \"__EVENTVALIDATION\": eventvalidation,\\n                          \"__VIEWSTATE\": viewstate\\n                    })\\n\\n    return r.text\\n\\n\\ndef test():\\n    data = extract_carriers(html_page)\\n#  assert len(data) == 16\\n#    assert \"FL\" in data\\n#    assert \"NK\" in data\\n\\ntest()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Please note that the function 'make_request' is provided for your reference only.\n",
    "You will not be able to to actually use it from within the Udacity web UI.\n",
    "All your changes should be in the 'extract_carrier' function.\n",
    "Also note that the html file is a stripped down version of what is actually on\n",
    "the website.\n",
    "\n",
    "Your task in this exercise is to get a list of all airlines. Exclude all of the\n",
    "combination values like \"All U.S. Carriers\" from the data that you return.\n",
    "You should return a list of codes for the carriers.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "#def extract_carriers(page):\n",
    "data = []\n",
    "\n",
    "with open(html_page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    option_values=[]\n",
    "    carrier_list = soup.find(id=\"CarrierList\")\n",
    "    print carrier_list\n",
    "    for option in carrier_list.find_all('option'):\n",
    "         if(option[\"value\"] not in [\"All\", \"AllUS\",\"AllForeigner\",\"AllMajors\",\"AllOthers\"]):\n",
    "            print \"---------------\"\n",
    "            print (option['value'])\n",
    "            data.append(option['value'])\n",
    "\"\"\"\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': airport,\n",
    "                          'CarrierList': carrier,\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "#  assert len(data) == 16\n",
    "#    assert \"FL\" in data\n",
    "#    assert \"NK\" in data\n",
    "\n",
    "test()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<select class=\"slcBox\" id=\"AirportList\" name=\"AirportList\" style=\"width:250px;\">\n",
      "<option selected=\"selected\" value=\"All\">All</option>\n",
      "<option value=\"AllMajors\">All Major Airports</option>\n",
      "<option value=\"ATL\"> - Atlanta, GA: Hartsfield-Jackson Atlanta International</option>\n",
      "<option value=\"BWI\"> - Baltimore, MD: Baltimore/Washington International Thurgood Marshall</option>\n",
      "<option value=\"BOS\"> - Boston, MA: Logan International</option>\n",
      "<option value=\"CLT\"> - Charlotte, NC: Charlotte Douglas International</option>\n",
      "<option value=\"MDW\"> - Chicago, IL: Chicago Midway International</option>\n",
      "<option value=\"ORD\"> - Chicago, IL: Chicago O'Hare International</option>\n",
      "<option value=\"DFW\"> - Dallas/Fort Worth, TX: Dallas/Fort Worth International</option>\n",
      "<option value=\"DEN\"> - Denver, CO: Denver International</option>\n",
      "<option value=\"DTW\"> - Detroit, MI: Detroit Metro Wayne County</option>\n",
      "<option value=\"FLL\"> - Fort Lauderdale, FL: Fort Lauderdale-Hollywood International</option>\n",
      "<option value=\"IAH\"> - Houston, TX: George Bush Intercontinental/Houston</option>\n",
      "<option value=\"LAS\"> - Las Vegas, NV: McCarran International</option>\n",
      "<option value=\"LAX\"> - Los Angeles, CA: Los Angeles International</option>\n",
      "<option value=\"AllOthers\">All Other Airports</option>\n",
      "<option value=\"ABR\"> - Aberdeen, SD: Aberdeen Regional</option>\n",
      "<option value=\"ABI\"> - Abilene, TX: Abilene Regional</option>\n",
      "</select>\n",
      "---------------\n",
      "ATL\n",
      "---------------\n",
      "BWI\n",
      "---------------\n",
      "BOS\n",
      "---------------\n",
      "CLT\n",
      "---------------\n",
      "MDW\n",
      "---------------\n",
      "ORD\n",
      "---------------\n",
      "DFW\n",
      "---------------\n",
      "DEN\n",
      "---------------\n",
      "DTW\n",
      "---------------\n",
      "FLL\n",
      "---------------\n",
      "IAH\n",
      "---------------\n",
      "LAS\n",
      "---------------\n",
      "LAX\n",
      "---------------\n",
      "ABR\n",
      "---------------\n",
      "ABI\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete the 'extract_airports' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "data = []\n",
    "with open(html_page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    option_values=[]\n",
    "    airport_list = soup.find(id=\"AirportList\")\n",
    "    print airport_list\n",
    "    for option in airport_list.find_all('option'):\n",
    "         if(option[\"value\"] not in [\"All\", \"AllUS\",\"AllForeigner\",\"AllMajors\",\"AllOthers\"]):\n",
    "            print \"---------------\"\n",
    "            print (option['value'])\n",
    "            data.append(option['value'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-c704bc74f0ad>, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-c704bc74f0ad>\"\u001b[1;36m, line \u001b[1;32m77\u001b[0m\n\u001b[1;33m    relevant_html = soup.find('table', class=\"dataTDRight\")\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "extract the flight data from that table as a list of dictionaries, each\n",
    "dictionary containing relevant data from the file and table row. This is an\n",
    "example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file' function.\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "    relevant_html = soup.find('table', class=\"dataTDRight\")\n",
    "    all_trs = relevant_html.find_all('tr')\n",
    "    for tr in all_trs:\n",
    "        if 'style' in tr.atts and tr['style'] ==\"color:White;background-color:#5D95C9;\": \n",
    "            pass\n",
    "        elif 'style' in tr.atts and tr['style'] == \"background-color:LightYellow;\": \n",
    "            pass\n",
    "        else:\n",
    "            row =[]\n",
    "            each_tr_info = {}\n",
    "            each_tr_info[\"courier\"] = info[\"courier\"]\n",
    "            each_tr_info[\"airport\"] = infor[\"airport\"]\n",
    "            for cell in tr.find_all('td'):\n",
    "                row.append(cell.text)\n",
    "    each_tr_infor[\"year\"] = int(float(row[0]))\n",
    "    each_tr_info[\"month\"] = int(float(row[1]))\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    print \"Running a simple test...\"\n",
    "    open_zip(datadir)\n",
    "    files = process_all(datadir)\n",
    "    data = []\n",
    "    # Test will loop over three data files.\n",
    "    for f in files:\n",
    "        data += process_file(f)\n",
    "        \n",
    "    assert len(data) == 399  # Total number of rows\n",
    "    for entry in data[:3]:\n",
    "        assert type(entry[\"year\"]) == int\n",
    "        assert type(entry[\"month\"]) == int\n",
    "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
    "        assert len(entry[\"airport\"]) == 3\n",
    "        assert len(entry[\"courier\"]) == 2\n",
    "    assert data[0][\"courier\"] == 'FL'\n",
    "    assert data[0][\"month\"] == 10\n",
    "    assert data[-1][\"airport\"] == \"ATL\"\n",
    "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
    "    \n",
    "    print \"... success!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This and the following exercise are using US Patent database. The patent.data\n",
    "file is a small excerpt of much larger datafiles that are available for\n",
    "download from US Patent website. These files are pretty large ( >100 MB each).\n",
    "The original file is ~600MB large, you might not be able to open it in a text\n",
    "editor.\n",
    "\n",
    "The data itself is in XML, however there is a problem with how it's formatted.\n",
    "Please run this script and observe the error. Then find the line that is\n",
    "causing the error. You can do that by just looking at the datafile in the web\n",
    "UI, or programmatically. For quiz purposes it does not matter, but as an\n",
    "exercise we suggest that you try to do it programmatically.\n",
    "\n",
    "NOTE: You do not need to correct the error - for now, just find where the error\n",
    "is occurring.\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "\n",
    "    tree = ET.parse(fname)\n",
    "    count = 0\n",
    "    for i,j in enumerate(f): \n",
    "        if j[:5] == \"<?xml\": \n",
    "            f1 =  open(\"{}-{}\".format(PATENTS, count), \"wb\") \n",
    "            f1.write(j.strip())\n",
    "            count += 1\n",
    "        else:\n",
    "            f1.write(j.strip())\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "get_root(PATENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# So, the problem is that the gigantic file is actually not a valid XML, because\n",
    "# it has several root elements, and XML declarations.\n",
    "# It is, a matter of fact, a collection of a lot of concatenated XML documents.\n",
    "# So, one solution would be to split the file into separate documents,\n",
    "# so that you can process the resulting files as valid XML documents.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def split_file(filename):\n",
    "    \"\"\"\n",
    "    Split the input file into separate files, each containing a single patent.\n",
    "    As a hint - each patent declaration starts with the same line that was\n",
    "    causing the error found in the previous exercises.\n",
    "    \n",
    "    The new files should be saved with filename in the following format:\n",
    "    \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def test():\n",
    "    split_file(PATENTS)\n",
    "    for n in range(4):\n",
    "        try:\n",
    "            fname = \"{}-{}\".format(PATENTS, n)\n",
    "            f = open(fname, \"r\")\n",
    "            if not f.readline().startswith(\"<?xml\"):\n",
    "                print \"You have not split the file {} in the correct boundary!\".format(fname)\n",
    "            f.close()\n",
    "        except:\n",
    "            print \"Could not find file {}. Check if the filename is correct!\".format(fname)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "- list, if the value starts with \"{\"\n",
    "- int, if the value can be cast to int\n",
    "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "   For example, '3.23e+07' should be considered a float because it can be cast\n",
    "   as float but int('3.23e+07') will throw a ValueError\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and a \n",
    "SET of the types that can be found in the field. e.g.\n",
    "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
    " \"field2\": set([type(str())]),\n",
    "  ....\n",
    "}\n",
    "The type() function returns a type object describing the argument given to the \n",
    "function. You can also use examples of objects to create type objects, e.g.\n",
    "type(1.1) for a float: see the test function below for examples.\n",
    "\n",
    "Note that the first three rows (after the header row) in the cities.csv file\n",
    "are not actual data points. The contents of these rows should note be included\n",
    "when processing data types. Be sure to include functionality in your code to\n",
    "skip over or detect these rows.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "\n",
    "def audit_file(filename, fields):\n",
    "    fieldtypes = {}, \"r\"\n",
    "    with open (filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i in range(0, len(fields)):\n",
    "            field = fields[i]\n",
    "            data_types = []\n",
    "            for row in reader:\n",
    "                data_types.append(type(row[field]))\n",
    "                unique_data_types = set(data_types)\n",
    "            fieldtype[field] = unique_data_types \n",
    "                for field in fields:\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    return fieldtypes\n",
    "\n",
    "\n",
    "def test():\n",
    "    fieldtypes = audit_file(CITIES, FIELDS)\n",
    "\n",
    "    pprint.pprint(fieldtypes)\n",
    "\n",
    "    assert fieldtypes[\"areaLand\"] == set([type(1.1), type([]), type(None)])\n",
    "    assert fieldtypes['areaMetro'] == set([type(1.1), type(None)])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "Since in the previous quiz you made a decision on which value to keep for the\n",
    "\"areaLand\" field, you now know what has to be done.\n",
    "\n",
    "Finish the function fix_area(). It will receive a string as an input, and it\n",
    "has to return a float representing the value of the area or None.\n",
    "You have to change the function fix_area. You can use extra functions if you\n",
    "like, but changes to process_file will not be taken into account.\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "\n",
    "def fix_area(area):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    # CHANGES TO THIS FUNCTION WILL BE IGNORED WHEN YOU SUBMIT THE EXERCISE\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"areaLand\" in line:\n",
    "                line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "            data.append(line)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "\n",
    "    print \"Printing three example results:\"\n",
    "    for n in range(5,8):\n",
    "        pprint.pprint(data[n][\"areaLand\"])\n",
    "\n",
    "    assert data[3][\"areaLand\"] == None        \n",
    "    assert data[8][\"areaLand\"] == 55166700.0\n",
    "    assert data[20][\"areaLand\"] == 14581600.0\n",
    "    assert data[33][\"areaLand\"] == 20564500.0    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "In the previous quiz you recognized that the \"name\" value can be an array (or\n",
    "list in Python terms). It would make it easier to process and query the data\n",
    "later if all values for the name are in a Python list, instead of being\n",
    "just a string separated with special characters, like now.\n",
    "\n",
    "Finish the function fix_name(). It will recieve a string as an input, and it\n",
    "will return a list of all the names. If there is only one name, the list will\n",
    "have only one item in it; if the name is \"NULL\", the list should be empty.\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "\n",
    "def fix_name(name):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"name\" in line:\n",
    "                line[\"name\"] = fix_name(line[\"name\"])\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "\n",
    "    print \"Printing 20 results:\"\n",
    "    for n in range(20):\n",
    "        pprint.pprint(data[n][\"name\"])\n",
    "\n",
    "    assert data[14][\"name\"] == ['Negtemiut', 'Nightmute']\n",
    "    assert data[9][\"name\"] == ['Pell City Alabama']\n",
    "    assert data[3][\"name\"] == ['Kumhari']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "If you look at the full city data, you will notice that there are couple of\n",
    "values that seem to provide the same information in different formats: \"point\"\n",
    "seems to be the combination of \"wgs84_pos#lat\" and \"wgs84_pos#long\". However,\n",
    "we do not know if that is the case and should check if they are equivalent.\n",
    "\n",
    "Finish the function check_loc(). It will recieve 3 strings: first, the combined\n",
    "value of \"point\" followed by the separate \"wgs84_pos#\" values. You have to\n",
    "extract the lat and long values from the \"point\" argument and compare them to\n",
    "the \"wgs84_pos# values, returning True or False.\n",
    "\n",
    "Note that you do not have to fix the values, only determine if they are\n",
    "consistent. To fix them in this case you would need more information. Feel free\n",
    "to discuss possible strategies for fixing this on the discussion forum.\n",
    "\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "Changes to \"process_file\" function will not be taken into account for grading.\n",
    "\"\"\"\n",
    "import csv\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "\n",
    "def check_loc(point, lat, longi):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        #skipping the extra matadata\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to check the location\n",
    "            result = check_loc(line[\"point\"], line[\"wgs84_pos#lat\"], line[\"wgs84_pos#long\"])\n",
    "            if not result:\n",
    "                print \"{}: {} != {} {}\".format(line[\"name\"], line[\"point\"], line[\"wgs84_pos#lat\"], line[\"wgs84_pos#long\"])\n",
    "            data.append(line)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    assert check_loc(\"33.08 75.28\", \"33.08\", \"75.28\") == True\n",
    "    assert check_loc(\"44.57833333333333 -91.21833333333333\", \"44.5783\", \"-91.2183\") == False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with another type of infobox data, audit it,\n",
    "clean it, come up with a data model, insert it into MongoDB and then run some\n",
    "queries against your database. The set contains data about Arachnid class\n",
    "animals.\n",
    "\n",
    "Your task in this exercise is to parse the file, process only the fields that\n",
    "are listed in the FIELDS dictionary as keys, and return a list of dictionaries\n",
    "of cleaned values. \n",
    "\n",
    "The following things should be done:\n",
    "- keys of the dictionary changed according to the mapping in FIELDS dictionary\n",
    "- trim out redundant description in parenthesis from the 'rdf-schema#label'\n",
    "  field, like \"(spider)\"\n",
    "- if 'name' is \"NULL\" or contains non-alphanumeric characters, set it to the\n",
    "  same value as 'label'.\n",
    "- if a value of a field is \"NULL\", convert it to None\n",
    "- if there is a value in 'synonym', it should be converted to an array (list)\n",
    "  by stripping the \"{}\" characters and splitting the string on \"|\". Rest of the\n",
    "  cleanup is up to you, e.g. removing \"*\" prefixes etc. If there is a singular\n",
    "  synonym, the value should still be formatted in a list.\n",
    "- strip leading and ending whitespace from all fields, if there is any\n",
    "- the output structure should be as follows:\n",
    "\n",
    "[ { 'label': 'Argiope',\n",
    "    'uri': 'http://dbpedia.org/resource/Argiope_(spider)',\n",
    "    'description': 'The genus Argiope includes rather large and spectacular spiders that often ...',\n",
    "    'name': 'Argiope',\n",
    "    'synonym': [\"One\", \"Two\"],\n",
    "    'classification': {\n",
    "                      'family': 'Orb-weaver spider',\n",
    "                      'class': 'Arachnid',\n",
    "                      'phylum': 'Arthropod',\n",
    "                      'order': 'Spider',\n",
    "                      'kingdom': 'Animal',\n",
    "                      'genus': None\n",
    "                      }\n",
    "  },\n",
    "  { 'label': ... , }, ...\n",
    "]\n",
    "\n",
    "  * Note that the value associated with the classification key is a dictionary\n",
    "    with taxonomic labels.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "DATAFILE = 'arachnid.csv'\n",
    "FIELDS ={'rdf-schema#label': 'label',\n",
    "         'URI': 'uri',\n",
    "         'rdf-schema#comment': 'description',\n",
    "         'synonym': 'synonym',\n",
    "         'name': 'name',\n",
    "         'family_label': 'family',\n",
    "         'class_label': 'class',\n",
    "         'phylum_label': 'phylum',\n",
    "         'order_label': 'order',\n",
    "         'kingdom_label': 'kingdom',\n",
    "         'genus_label': 'genus'}\n",
    "\n",
    "\n",
    "def process_file(filename, fields):\n",
    "\n",
    "    process_fields = fields.keys()\n",
    "    data = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i in range(3):\n",
    "            l = reader.next()\n",
    "\n",
    "        for line in reader:\n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_array(v):\n",
    "    if (v[0] == \"{\") and (v[-1] == \"}\"):\n",
    "        v = v.lstrip(\"{\")\n",
    "        v = v.rstrip(\"}\")\n",
    "        v_array = v.split(\"|\")\n",
    "        v_array = [i.strip() for i in v_array]\n",
    "        return v_array\n",
    "    return [v]\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(DATAFILE, FIELDS)\n",
    "    print \"Your first entry:\"\n",
    "    pprint.pprint(data[0])\n",
    "    first_entry = {\n",
    "        \"synonym\": None, \n",
    "        \"name\": \"Argiope\", \n",
    "        \"classification\": {\n",
    "            \"kingdom\": \"Animal\", \n",
    "            \"family\": \"Orb-weaver spider\", \n",
    "            \"order\": \"Spider\", \n",
    "            \"phylum\": \"Arthropod\", \n",
    "            \"genus\": None, \n",
    "            \"class\": \"Arachnid\"\n",
    "        }, \n",
    "        \"uri\": \"http://dbpedia.org/resource/Argiope_(spider)\", \n",
    "        \"label\": \"Argiope\", \n",
    "        \"description\": \"The genus Argiope includes rather large and spectacular spiders that often have a strikingly coloured abdomen. These spiders are distributed throughout the world. Most countries in tropical or temperate climates host one or more species that are similar in appearance. The etymology of the name is from a Greek name meaning silver-faced.\"\n",
    "    }\n",
    "\n",
    "    assert len(data) == 76\n",
    "    assert data[0] == first_entry\n",
    "    assert data[17][\"name\"] == \"Ogdenia\"\n",
    "    assert data[48][\"label\"] == \"Hydrachnidiae\"\n",
    "    assert data[14][\"synonym\"] == [\"Cyrene Peckham & Peckham\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task is to use the iterative parsing to process the map file and\n",
    "find out not only what tags are there, but also how many, to get the\n",
    "feeling on how much of which data you can expect to have in the map.\n",
    "Fill out the count_tags function. It should return a dictionary with the \n",
    "tag name as the key and number of times this tag can be encountered in \n",
    "the map as value.\n",
    "\n",
    "Note that your code will be tested with a different data file than the 'example.osm'\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "        # YOUR CODE HERE\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tags.keys():\n",
    "            tags[elem.tag]=1\n",
    "        else:\n",
    "            tags[elem.tag] +=1\n",
    "        return tags\n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('example.osm')\n",
    "    pprint.pprint(tags)\n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "Before you process the data and add it into your database, you should check the\n",
    "\"k\" value for each \"<tag>\" and see if there are any potential problems.\n",
    "\n",
    "We have provided you with 3 regular expressions to check for certain patterns\n",
    "in the tags. As we saw in the quiz earlier, we would like to change the data\n",
    "model and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and if we have any tags with\n",
    "problematic characters.\n",
    "\n",
    "Please complete the function 'key_type', such that we have a count of each of\n",
    "four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n",
    "See the 'process_map' and 'test' functions for examples of the expected format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        # YOUR CODE HERE\n",
    "        k = element.attrib['k']\n",
    "        if re.match(lower,k):\n",
    "            keys['lower'] +=1\n",
    "        elif re.match(lower_colon,k):\n",
    "            keys['lower_colon'] +=1\n",
    "        elif re.match(problemchars,k):\n",
    "            keys['problemchars'] +=1\n",
    "        else:\n",
    "            keys[\"other\"] +=1\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    # You can use another testfile 'map.osm' to look at your solution\n",
    "    # Note that the assertion below will be incorrect then.\n",
    "    # Note as well that the test function here is only used in the Test Run;\n",
    "    # when you submit, your code will be checked against a different dataset.\n",
    "    keys = process_map('example.osm')\n",
    "    pprint.pprint(keys)\n",
    "    assert keys == {'lower': 5, 'lower_colon': 0, 'other': 1, 'problemchars': 1}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "The first task is a fun one - find out how many unique users\n",
    "have contributed to the map in this particular area!\n",
    "\n",
    "The function process_map should return a set of unique user IDs (\"uid\")\n",
    "\"\"\"\n",
    "\n",
    "def get_user(element):\n",
    "    uid = \"\"\n",
    "    if element.tag == \"node\":\n",
    "        uid = element.get('uid')\n",
    "    elif element.tag == \"way\":\n",
    "        uid = element.get('uid')\n",
    "    elif element.tag == \"relation\":\n",
    "        uid = element.get('uid')\n",
    "    print uid\n",
    "    return uid\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        users.add(get_user(element))\n",
    "        users.discard('')\n",
    "    return users\n",
    "\n",
    "\n",
    "def test():\n",
    "\n",
    "    users = process_map('example.osm')\n",
    "    pprint.pprint(users)\n",
    "    assert len(users) == 6\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task in this exercise has two steps:\n",
    "\n",
    "- audit the OSMFILE and change the variable 'mapping' to reflect the changes needed to fix \n",
    "    the unexpected street types to the appropriate ones in the expected list.\n",
    "    You have to add mappings only for the actual problems you find in this OSMFILE,\n",
    "    not a generalized solution, since that may and will depend on the particular area you are auditing.\n",
    "- write the update_name function, to actually fix the street name.\n",
    "    The function takes a string with street name as an argument and should return the fixed name\n",
    "    We have provided a simple test so that you see what exactly is expected\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"example.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\"\n",
    "            }\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            name = re.sub(street_type_re, mapping[street_type], name)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    assert len(st_types) == 3\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name\n",
    "            if name == \"West Lexington St.\":\n",
    "                assert better_name == \"West Lexington Street\"\n",
    "            if name == \"Baldwin Rd.\":\n",
    "                assert better_name == \"Baldwin Road\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\"\"\"\n",
    "Your task is to wrangle the data and transform the shape of the data\n",
    "into the model we mentioned earlier. The output should be a list of dictionaries\n",
    "that look like this:\n",
    "\n",
    "{\n",
    "\"id\": \"2406124091\",\n",
    "\"type: \"node\",\n",
    "\"visible\":\"true\",\n",
    "\"created\": {\n",
    "          \"version\":\"2\",\n",
    "          \"changeset\":\"17206049\",\n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "          \"user\":\"linuxUser16\",\n",
    "          \"uid\":\"1219059\"\n",
    "        },\n",
    "\"pos\": [41.9757030, -87.6921867],\n",
    "\"address\": {\n",
    "          \"housenumber\": \"5157\",\n",
    "          \"postcode\": \"60625\",\n",
    "          \"street\": \"North Lincoln Ave\"\n",
    "        },\n",
    "\"amenity\": \"restaurant\",\n",
    "\"cuisine\": \"mexican\",\n",
    "\"name\": \"La Cabana De Don Luis\",\n",
    "\"phone\": \"1 (773)-271-5176\"\n",
    "}\n",
    "\n",
    "You have to complete the function 'shape_element'.\n",
    "We have provided a function that will parse the map file, and call the function with the element\n",
    "as an argument. You should return a dictionary, containing the shaped data for that element.\n",
    "We have also provided a way to save the data in a file, so that you could use\n",
    "mongoimport later on to import the shaped data into MongoDB. \n",
    "\n",
    "Note that in this exercise we do not use the 'update street name' procedures\n",
    "you worked on in the previous exercise. If you are using this code in your final\n",
    "project, you are strongly encouraged to use the code from previous exercise to \n",
    "update the street names before you save them to JSON. \n",
    "\n",
    "In particular the following things should be done:\n",
    "- you should process only 2 types of top level tags: \"node\" and \"way\"\n",
    "- all attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "    - attributes in the CREATED array should be added under a key \"created\"\n",
    "    - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "      for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "      and not strings. \n",
    "- if the second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "- if the second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "- if the second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can\n",
    "  process it in a way that you feel is best. For example, you might split it into a two-level\n",
    "  dictionary like with \"addr:\", or otherwise convert the \":\" to create a valid key.\n",
    "- if there is a second \":\" that separates the type/direction of a street,\n",
    "  the tag should be ignored, for example:\n",
    "\n",
    "<tag k=\"addr:housenumber\" v=\"5158\"/>\n",
    "<tag k=\"addr:street\" v=\"North Lincoln Avenue\"/>\n",
    "<tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "<tag k=\"addr:street:prefix\" v=\"North\"/>\n",
    "<tag k=\"addr:street:type\" v=\"Avenue\"/>\n",
    "<tag k=\"amenity\" v=\"pharmacy\"/>\n",
    "\n",
    "  should be turned into:\n",
    "\n",
    "{...\n",
    "\"address\": {\n",
    "    \"housenumber\": 5158,\n",
    "    \"street\": \"North Lincoln Avenue\"\n",
    "}\n",
    "\"amenity\": \"pharmacy\",\n",
    "...\n",
    "}\n",
    "\n",
    "- for \"way\" specifically:\n",
    "\n",
    "  <nd ref=\"305896090\"/>\n",
    "  <nd ref=\"1719825889\"/>\n",
    "\n",
    "should be turned into\n",
    "\"node_refs\": [\"305896090\", \"1719825889\"]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    address = {}\n",
    "    pos = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        # YOUR CODE HERE\n",
    "        node[\"id\"] = element.attrib[\"id\"]\n",
    "        node[\"type\"] = element.tag\n",
    "        node[\"visible\"] = element.get(\"visible\")\n",
    "        created = {}\n",
    "        created[\"version\"] = element.attrib[\"version\"]\n",
    "        created[\"changeset\"] = element.attrib[\"changeset\"]\n",
    "        created[\"timestamp\"] =  element.attrib[\"timestamp\"]\n",
    "        created[\"user\"] = element.attrib[\"user\"]\n",
    "        created[\"uid\"] = element.attrib[\"uid\"]\n",
    "        node[\"created\"] = created\n",
    "        if \"lat\" in element.keys() and \"lon\" in element.keys():\n",
    "            pos = [element.attrib[\"lat\"], element.attrib[\"lon\"]]\n",
    "            node[\"pos\"] = [float(string) for string in pos]\n",
    "            else:\n",
    "                node[\"pos\"] = None\n",
    "            for tag in element.iter('tag'):\n",
    "                if re.search('addr:',tag.attrib['k']):\n",
    "                    if len(tag.attrib['k'].split(\":\")) < 3:\n",
    "                        addr_add = tag.attrib['k'].split(\":\")[1]\n",
    "                        address[addr_add] = tag.attrib['v']\n",
    "            if address:\n",
    "                node['address'] = address\n",
    "            for nd in element.iter('nd'):\n",
    "                if not \"node_refs\" in node.key():\n",
    "                    node[\"node_refs\"] = []\n",
    "                node_refs = node[\"node_refs\"]\n",
    "                node_refs.append(nd.attrib[\"ref\"])\n",
    "                node[\"node_refs\"] = node_refs\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "def test():\n",
    "    # NOTE: if you are running this code on your computer, with a larger dataset, \n",
    "    # call the process_map procedure with pretty=False. The pretty=True option adds \n",
    "    # additional spaces to the output, making it significantly larger.\n",
    "    data = process_map('example.osm', True)\n",
    "    #pprint.pprint(data)\n",
    "    \n",
    "    correct_first_elem = {\n",
    "        \"id\": \"261114295\", \n",
    "        \"visible\": \"true\", \n",
    "        \"type\": \"node\", \n",
    "        \"pos\": [41.9730791, -87.6866303], \n",
    "        \"created\": {\n",
    "            \"changeset\": \"11129782\", \n",
    "            \"user\": \"bbmiller\", \n",
    "            \"version\": \"7\", \n",
    "            \"uid\": \"451048\", \n",
    "            \"timestamp\": \"2012-03-28T18:31:23Z\"\n",
    "        }\n",
    "    }\n",
    "    assert data[0] == correct_first_elem\n",
    "    assert data[-1][\"address\"] == {\n",
    "                                    \"street\": \"West Lexington St.\", \n",
    "                                    \"housenumber\": \"1412\"\n",
    "                                      }\n",
    "    assert data[-1][\"node_refs\"] == [ \"2199822281\", \"2199822390\",  \"2199822392\", \"2199822369\", \n",
    "                                    \"2199822370\", \"2199822284\", \"2199822281\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
